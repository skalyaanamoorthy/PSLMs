{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, append Rosetta predictions to inference files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s461\n",
      "461 461\n",
      "ssym\n",
      "684 684\n",
      "korpm\n",
      "2369 2369\n",
      "korpm_full\n",
      "3822 3822\n",
      "Could not find predictions for 1ACB_11I\n",
      "Could not find predictions for 1AJ3_62A\n",
      "Could not find predictions for 1AJ3_62G\n",
      "Could not find predictions for 1AJ3_76G\n",
      "Could not find predictions for 1AXB_263F\n",
      "Could not find predictions for 1AYI_16A\n",
      "Could not find predictions for 1AYI_22V\n",
      "Could not find predictions for 1AYI_34A\n",
      "Could not find predictions for 1AYI_38A\n",
      "Could not find predictions for 1BK7_101A\n",
      "Could not find predictions for 1BK7_102A\n",
      "Could not find predictions for 1BK7_105L\n",
      "Could not find predictions for 1BK7_107A\n",
      "Could not find predictions for 1BK7_125A\n",
      "Could not find predictions for 1BK7_127A\n",
      "Could not find predictions for 1BK7_166A\n",
      "Could not find predictions for 1BK7_173A\n",
      "Could not find predictions for 1BK7_190A\n",
      "Could not find predictions for 1BNI_39A\n",
      "Could not find predictions for 1BNZ_12A\n",
      "Could not find predictions for 1BNZ_15A\n",
      "Could not find predictions for 1BNZ_18A\n",
      "Could not find predictions for 1BNZ_23A\n",
      "Could not find predictions for 1BNZ_25A\n",
      "Could not find predictions for 1BNZ_27A\n",
      "Could not find predictions for 1BNZ_30A\n",
      "Could not find predictions for 1BNZ_35A\n",
      "Could not find predictions for 1BNZ_37A\n",
      "Could not find predictions for 1BNZ_41A\n",
      "Could not find predictions for 1BNZ_46A\n",
      "Could not find predictions for 1BNZ_4A\n",
      "Could not find predictions for 1BNZ_51G\n",
      "Could not find predictions for 1BNZ_56A\n",
      "Could not find predictions for 1BNZ_59A\n",
      "Could not find predictions for 1BNZ_7A\n",
      "Could not find predictions for 1BU4_29A\n",
      "Could not find predictions for 1BU4_9A\n",
      "Could not find predictions for 1BVC_133A\n",
      "Could not find predictions for 1BVC_139A\n",
      "Could not find predictions for 1BVC_147A\n",
      "Could not find predictions for 1BVC_20A\n",
      "Could not find predictions for 1C9O_66L\n",
      "Could not find predictions for 1CFD_41A\n",
      "Could not find predictions for 1CM2_49A\n",
      "Could not find predictions for 1DKT_20A\n",
      "Could not find predictions for 1EGL_14A\n",
      "Could not find predictions for 1FKJ_6A\n",
      "Could not find predictions for 1FTG_104A\n",
      "Could not find predictions for 1FTG_106G\n",
      "Could not find predictions for 1FTG_108A\n",
      "Could not find predictions for 1FTG_7F\n",
      "Could not find predictions for 1FTG_84F\n",
      "Could not find predictions for 1FTG_91A\n",
      "Could not find predictions for 1FTG_99N\n",
      "Could not find predictions for 1G3P_76I\n",
      "Could not find predictions for 1G6P_40A\n",
      "Could not find predictions for 1G6P_62A\n",
      "Could not find predictions for 1GLM_136A\n",
      "Could not find predictions for 1GLM_250A\n",
      "Could not find predictions for 1GLM_382A\n",
      "Could not find predictions for 1H7M_48M\n",
      "Could not find predictions for 1HFY_27A\n",
      "Could not find predictions for 1JNX_124S\n",
      "Could not find predictions for 1JNX_129N\n",
      "Could not find predictions for 1JNX_157A\n",
      "Could not find predictions for 1JNX_159A\n",
      "Could not find predictions for 1JNX_44N\n",
      "Could not find predictions for 1JNX_57A\n",
      "Could not find predictions for 1JNX_87A\n",
      "Could not find predictions for 1K5U_17A\n",
      "Could not find predictions for 1K5U_19A\n",
      "Could not find predictions for 1K5U_59A\n",
      "Could not find predictions for 1K5U_61A\n",
      "Could not find predictions for 1K85_20A\n",
      "Could not find predictions for 1K85_22A\n",
      "Could not find predictions for 1K85_33A\n",
      "Could not find predictions for 1K85_36F\n",
      "Could not find predictions for 1K85_38A\n",
      "Could not find predictions for 1K85_44A\n",
      "Could not find predictions for 1K85_48A\n",
      "Could not find predictions for 1K85_55A\n",
      "Could not find predictions for 1K85_58A\n",
      "Could not find predictions for 1K85_64F\n",
      "Could not find predictions for 1K85_66L\n",
      "Could not find predictions for 1K85_68A\n",
      "Could not find predictions for 1K85_70G\n",
      "Could not find predictions for 1K85_81A\n",
      "Could not find predictions for 1K85_84A\n",
      "Could not find predictions for 1K85_86A\n",
      "Could not find predictions for 1OIA_51A\n",
      "Could not find predictions for 1QQV_52G\n",
      "Could not find predictions for 1RGG_11A\n",
      "Could not find predictions for 1RGG_19A\n",
      "Could not find predictions for 1RGG_21A\n",
      "Could not find predictions for 1RGG_22V\n",
      "Could not find predictions for 1RGG_24A\n",
      "Could not find predictions for 1RGG_31A\n",
      "Could not find predictions for 1RGG_3A\n",
      "Could not find predictions for 1RGG_42A\n",
      "Could not find predictions for 1RGG_70V\n",
      "Could not find predictions for 1RGG_71V\n",
      "Could not find predictions for 1RGG_8A\n",
      "Could not find predictions for 1RGG_90A\n",
      "Could not find predictions for 1RGG_91A\n",
      "Could not find predictions for 1RGG_92V\n",
      "Could not find predictions for 1RGG_95A\n",
      "Could not find predictions for 1RGG_9A\n",
      "Could not find predictions for 1RTB_83A\n",
      "Could not find predictions for 1RX4_136A\n",
      "Could not find predictions for 1RX4_156A\n",
      "Could not find predictions for 1RX4_4A\n",
      "Could not find predictions for 1RX4_61A\n",
      "Could not find predictions for 1RX4_8A\n",
      "Could not find predictions for 1RX4_91A\n",
      "Could not find predictions for 1U4Q_102G\n",
      "Could not find predictions for 1U4Q_104A\n",
      "Could not find predictions for 1U4Q_107A\n",
      "Could not find predictions for 1U4Q_110A\n",
      "Could not find predictions for 1U4Q_12G\n",
      "Could not find predictions for 1U4Q_14A\n",
      "Could not find predictions for 1U4Q_18A\n",
      "Could not find predictions for 1U4Q_20A\n",
      "Could not find predictions for 1U4Q_21A\n",
      "Could not find predictions for 1U4Q_22A\n",
      "Could not find predictions for 1U4Q_25A\n",
      "Could not find predictions for 1U4Q_35G\n",
      "Could not find predictions for 1U4Q_37A\n",
      "Could not find predictions for 1U4Q_40A\n",
      "Could not find predictions for 1U4Q_42A\n",
      "Could not find predictions for 1U4Q_46A\n",
      "Could not find predictions for 1U4Q_47A\n",
      "Could not find predictions for 1U4Q_49G\n",
      "Could not find predictions for 1U4Q_51A\n",
      "Could not find predictions for 1U4Q_54A\n",
      "Could not find predictions for 1U4Q_56A\n",
      "Could not find predictions for 1U4Q_58A\n",
      "Could not find predictions for 1U4Q_59A\n",
      "Could not find predictions for 1U4Q_5A\n",
      "Could not find predictions for 1U4Q_61A\n",
      "Could not find predictions for 1U4Q_65G\n",
      "Could not find predictions for 1U4Q_67A\n",
      "Could not find predictions for 1U4Q_67G\n",
      "Could not find predictions for 1U4Q_77A\n",
      "Could not find predictions for 1U4Q_79A\n",
      "Could not find predictions for 1U4Q_81A\n",
      "Could not find predictions for 1U4Q_83A\n",
      "Could not find predictions for 1U4Q_84A\n",
      "Could not find predictions for 1U4Q_86A\n",
      "Could not find predictions for 1U4Q_90A\n",
      "Could not find predictions for 1U4Q_91G\n",
      "Could not find predictions for 1U4Q_93A\n",
      "Could not find predictions for 1U4Q_97G\n",
      "Could not find predictions for 1U4Q_99G\n",
      "Could not find predictions for 1VII_11A\n",
      "Could not find predictions for 1VII_18A\n",
      "Could not find predictions for 1VII_21A\n",
      "Could not find predictions for 1VII_29A\n",
      "Could not find predictions for 1VII_35A\n",
      "Could not find predictions for 1VII_7A\n",
      "Could not find predictions for 1W4H_10V\n",
      "Could not find predictions for 1W4H_14A\n",
      "Could not find predictions for 1W4H_27S\n",
      "Could not find predictions for 1W4H_33A\n",
      "Could not find predictions for 1W4H_37N\n",
      "Could not find predictions for 1W4H_38A\n",
      "Could not find predictions for 1W4H_43G\n",
      "Could not find predictions for 1W4H_5G\n",
      "Could not find predictions for 1WQ5_234A\n",
      "Could not find predictions for 1WQ5_87A\n",
      "Could not find predictions for 1WQ5_92A\n",
      "Could not find predictions for 1WQ5_94A\n",
      "Could not find predictions for 1WY3_13V\n",
      "Could not find predictions for 1WY3_2A\n",
      "Could not find predictions for 1XYX_76K\n",
      "Could not find predictions for 1YGW_91A\n",
      "Could not find predictions for 1YGW_91V\n",
      "Could not find predictions for 1YGW_93A\n",
      "Could not find predictions for 1YGW_93V\n",
      "Could not find predictions for 1YRI_17A\n",
      "Could not find predictions for 1YRI_9A\n",
      "Could not find predictions for 2CK2_20A\n",
      "Could not find predictions for 2CK2_20V\n",
      "Could not find predictions for 2CK2_25A\n",
      "Could not find predictions for 2CK2_36A\n",
      "Could not find predictions for 2CK2_36F\n",
      "Could not find predictions for 2CK2_38A\n",
      "Could not find predictions for 2CK2_59A\n",
      "Could not find predictions for 2CK2_5A\n",
      "Could not find predictions for 2CK2_62A\n",
      "Could not find predictions for 2CK2_68F\n",
      "Could not find predictions for 2JOF_14A\n",
      "Could not find predictions for 2JOF_17A\n",
      "Could not find predictions for 2JOF_18A\n",
      "Could not find predictions for 2JOF_19A\n",
      "Could not find predictions for 2LCP_30F\n",
      "Could not find predictions for 2LSB_89N\n",
      "Could not find predictions for 2MMX_2L\n",
      "Could not find predictions for 2MMX_2W\n",
      "Could not find predictions for 2N53_45A\n",
      "Could not find predictions for 2RLK_13A\n",
      "Could not find predictions for 2RLK_14A\n",
      "Could not find predictions for 2RLK_21A\n",
      "Could not find predictions for 2RLK_27A\n",
      "Could not find predictions for 2RN4_10A\n",
      "Could not find predictions for 2SPZ_29G\n",
      "Could not find predictions for 2WZB_240A\n",
      "Could not find predictions for 3F6R_108A\n",
      "Could not find predictions for 3F6R_119A\n",
      "Could not find predictions for 3F6R_123A\n",
      "Could not find predictions for 3F6R_136A\n",
      "Could not find predictions for 3F6R_18G\n",
      "Could not find predictions for 3F6R_32A\n",
      "Could not find predictions for 3F6R_35A\n",
      "Could not find predictions for 3F6R_4A\n",
      "Could not find predictions for 3F6R_52A\n",
      "Could not find predictions for 3F6R_54A\n",
      "Could not find predictions for 3F6R_6A\n",
      "Could not find predictions for 3F6R_73A\n",
      "Could not find predictions for 3F6R_87A\n",
      "Could not find predictions for 3FFN_165N\n",
      "Could not find predictions for 5TR5_44A\n",
      "Could not find predictions for 9RNT_61V\n",
      "Could not find predictions for 9RNT_86A\n",
      "Could not find predictions for 9RNT_90V\n",
      "q3421\n",
      "3421 3421\n",
      "fireprot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2185826/1942450247.py:19: DtypeWarning: Columns (109,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  db = pd.read_csv(file1, index_col=0)\n",
      "/tmp/ipykernel_2185826/1942450247.py:24: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  db2 = pd.read_csv(file1.replace('_preds', '').replace('inference', 'preprocessed'), index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6455 53430\n",
      "Could not find predictions for 1AJ3_-1000000A\n",
      "Could not find predictions for 1AJ3_-1000000E\n",
      "Could not find predictions for 1AJ3_-1000000F\n",
      "Could not find predictions for 1AJ3_-1000000G\n",
      "Could not find predictions for 1AJ3_-1000000K\n",
      "Could not find predictions for 1AJ3_-1000000L\n",
      "Could not find predictions for 1AJ3_-1000000V\n",
      "Could not find predictions for 1AJ3_-1000000Y\n",
      "Could not find predictions for 1DPM_257L\n",
      "Could not find predictions for 1E21_-1000000A\n",
      "Could not find predictions for 1HYN_-1000000K\n",
      "Could not find predictions for 1PGA_-1000000F\n",
      "Could not find predictions for 1PGA_-1000000I\n",
      "Could not find predictions for 1PGA_-1000000V\n",
      "Could not find predictions for 1PGA_-1000000Y\n",
      "Could not find predictions for 1PIN_-1000000A\n",
      "Could not find predictions for 1PIN_-1000000G\n",
      "Could not find predictions for 1QLP_-1000000I\n",
      "Could not find predictions for 1QLP_-1000000L\n",
      "Could not find predictions for 1QLP_-1000000P\n",
      "Could not find predictions for 1QLP_-1000000V\n",
      "Could not find predictions for 1STN_-1000000A\n",
      "Could not find predictions for 1STN_-1000000F\n",
      "Could not find predictions for 1STN_-1000000G\n",
      "Could not find predictions for 1STN_-1000000K\n",
      "Could not find predictions for 1STN_-1000000N\n",
      "Could not find predictions for 1STN_-1000000V\n",
      "Could not find predictions for 1TTG_-1000000P\n",
      "Could not find predictions for 1TUP_-1000000A\n",
      "Could not find predictions for 1TUP_-1000000F\n",
      "Could not find predictions for 1TUP_-1000000G\n",
      "Could not find predictions for 1TUP_-1000000I\n",
      "Could not find predictions for 1TUP_-1000000L\n",
      "Could not find predictions for 1TUP_-1000000V\n",
      "Could not find predictions for 1UHG_-1000000A\n",
      "Could not find predictions for 1WQ5_-1000000A\n",
      "Could not find predictions for 1WQ5_-1000000G\n",
      "Could not find predictions for 1WQ5_-1000000K\n",
      "Could not find predictions for 1WQ5_-1000000Q\n",
      "Could not find predictions for 2O9P_373R\n",
      "Could not find predictions for 2WSY_-1000000A\n",
      "Could not find predictions for 3D2A_112P\n",
      "Could not find predictions for 3D2A_130D\n",
      "Could not find predictions for 3D2A_155M\n",
      "Could not find predictions for 4E5K_130K\n",
      "Could not find predictions for 4E5K_130Q\n",
      "Could not find predictions for 4E5K_130R\n",
      "Could not find predictions for 4E5K_132K\n",
      "Could not find predictions for 4E5K_132R\n",
      "Could not find predictions for 4E5K_137H\n",
      "Could not find predictions for 4E5K_137R\n",
      "Could not find predictions for 4E5K_150F\n",
      "Could not find predictions for 4E5K_215L\n",
      "Could not find predictions for 4E5K_215M\n",
      "Could not find predictions for 4E5K_275L\n",
      "Could not find predictions for 4E5K_275Q\n",
      "Could not find predictions for 4E5K_276C\n",
      "Could not find predictions for 4E5K_276H\n",
      "Could not find predictions for 4E5K_276Q\n",
      "Could not find predictions for 4E5K_276R\n",
      "Could not find predictions for 4E5K_276S\n",
      "Could not find predictions for 4E5K_313L\n",
      "Could not find predictions for 4E5K_315A\n",
      "Could not find predictions for 4E5K_319E\n",
      "Could not find predictions for 4E5K_325V\n",
      "Could not find predictions for 4E5K_71I\n",
      "Could not find predictions for 6BQG_352N\n"
     ]
    }
   ],
   "source": [
    "import analysis_utils\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "\n",
    "# open each of the main dataset inference files\n",
    "for file1 in ['../data/inference/s461_mapped_preds.csv',\n",
    "              '../data/inference/ssym_mapped_preds.csv',\n",
    "              '../data/inference/korpm_mapped_preds.csv',\n",
    "              '../data/inference/korpm_full_mapped_preds.csv', \n",
    "              '../data/inference/q3421_mapped_preds.csv',\n",
    "              '../data/inference/fireprot_mapped_preds.csv']:  \n",
    "\n",
    "    dataset = file1.split('/')[-1].split('_mapped')[0]\n",
    "    print(dataset)\n",
    "\n",
    "    db = pd.read_csv(file1, index_col=0)\n",
    "    # two entries get inexplicably duplicated in korpm datasets\n",
    "    # but only two inconsequential columns are different\n",
    "    db = db.loc[~db.index.duplicated(keep='last')]\n",
    "\n",
    "    db['uid2'] = db['code'] + '_' + db['position'].fillna(-1000000).astype(int).astype(str) + db['mutation']\n",
    "    \n",
    "    db2 = pd.read_csv(file1.replace('_preds', '').replace('inference', 'preprocessed'), index_col=0)\n",
    "    print(len(db), len(db2))\n",
    "    #assert len(db) == len(db2)\n",
    "\n",
    "    # add the (organism superfamily) origin column if missing\n",
    "    if not 'origin' in db.columns and dataset!='fireprot':\n",
    "        db = db.join(db2['origin'])\n",
    "    elif not 'origin' in db.columns:\n",
    "        db['origin'] = list(db2['origin'])\n",
    "\n",
    "    # replace the cartesian_ddg predictions in case they were updated\n",
    "    if 'cartesian_ddg_dir' in db.columns:\n",
    "        db = db.drop(['cartesian_ddg_dir', 'runtime_cartesian_ddg_dir'], axis=1)\n",
    "    if 'Unnamed: 0' in db.columns:\n",
    "        db = db.drop(['Unnamed: 0'], axis=1)\n",
    "    \n",
    "    db_runtimes = db[[c for c in db.columns if 'runtime' in c or 'uid2' == c]]\n",
    "\n",
    "    db = db.reset_index().rename({'uid': 'uid_'}, axis=1).rename({'uid2': 'uid'}, axis=1).set_index('uid')\n",
    "    # extract the runtimes for methods that have it (not currently used)\n",
    "    db_runtimes = db_runtimes.reset_index().rename({'uid': 'uid_'}, axis=1).rename({'uid2': 'uid'}, axis=1).set_index('uid')\n",
    "    # assuming you have designated the repo location as the path\n",
    "    df_cart, df_cart_runtimes = analysis_utils.parse_rosetta_predictions(db, os.path.join('..', 'data', 'rosetta_predictions'), runtime=True)\n",
    "    \n",
    "    db_mod = db.copy(deep=True)\n",
    "\n",
    "    # juggle the indices if needed\n",
    "\n",
    "    db_mod = db_mod.join(df_cart.astype(float), how='left')\n",
    "    db_mod = db_mod.join(df_cart_runtimes[['runtime_cartesian_ddg_dir']], how='left')\n",
    "    db_mod.index.name = 'uid'\n",
    "    db_mod = db_mod.reset_index().rename({'uid': 'uid2'}, axis=1).rename({'uid_': 'uid'}, axis=1).set_index(['uid', 'uid2'])\n",
    "\n",
    "    db_mod.to_csv(file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, add the features that will be used in analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute all pairs of structures (for running FATCAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731\n",
      "389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2185826/306366449.py:2: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  fireprot = pd.read_csv('../data/preprocessed/fireprot_mapped.csv', index_col=0)\n"
     ]
    }
   ],
   "source": [
    "# load the original mapped databases (from preprocessing)\n",
    "fireprot = pd.read_csv('../data/preprocessed/fireprot_mapped.csv', index_col=0)\n",
    "s461 = pd.read_csv('../data/preprocessed/s461_mapped.csv', index_col=0)\n",
    "s669 = pd.read_csv('../data/preprocessed/s669_mapped.csv', index_col=0)\n",
    "q3421 = pd.read_csv('../data/preprocessed/q3421_mapped.csv', index_col=0)\n",
    "ssym = pd.read_csv('../data/preprocessed/ssym_mapped.csv', index_col=0)\n",
    "korpm = pd.read_csv('../data/preprocessed/korpm_full_mapped.csv', index_col=0)\n",
    "\n",
    "all_structs = set()\n",
    "\n",
    "# add the unique structures from each database to a set\n",
    "for df in [fireprot, s461, s669, q3421, korpm]:\n",
    "    df['structure'] = df['code'] + '_' + df['chain']\n",
    "    for s in df['structure'].unique():\n",
    "        all_structs.add(s)\n",
    "\n",
    "# for ssym, we are only going to use the forward (not reverse/mutant) structures \n",
    "ssym['structure'] = ssym['wt_code'] + '_' + ssym['chain']\n",
    "for s in ssym['structure'].unique():\n",
    "    all_structs.add(s)\n",
    "\n",
    "# make a separate set that does include the mutant structures as well\n",
    "all_structs_mutant = copy.deepcopy(all_structs)\n",
    "\n",
    "# add the mutant structures to this\n",
    "ssym['structure2'] = ssym['code'] + '_' + ssym['chain']\n",
    "for s in ssym['structure2'].unique():\n",
    "    all_structs_mutant.add(s)\n",
    "\n",
    "# make a sorted list from the set\n",
    "all_structs_mutant = sorted(list(all_structs_mutant))\n",
    "all_structs_mutant = [s[:4] for s in all_structs_mutant]\n",
    "print(len(all_structs_mutant))\n",
    "\n",
    "# save to a convenience file that shows all PDBs used in this study\n",
    "with open('../data/all_structs.txt', 'w') as f:\n",
    "    for struct1 in all_structs_mutant:\n",
    "       f.write(f'{struct1}\\n') \n",
    "\n",
    "# make a sorted list of the wild-type structures\n",
    "all_structs = sorted(list(all_structs))\n",
    "print(len(all_structs))\n",
    "\n",
    "# match each structure to each other for FATCAT structural alignment\n",
    "with open('../data/all_pairs.txt', 'w') as f:    \n",
    "    for struct1 in all_structs:\n",
    "        for struct2 in all_structs:\n",
    "            if struct1 != struct2:\n",
    "                f.write(f'{struct1} {struct2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse results from FATCAT (expected at ../data/homology/structural_homology.aln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       code_1 chain_1 code_2 chain_2  P-value  Afp-num  Identity (%)  \\\n",
      "0        12CA       A   1A0F       A   0.9350    11638          3.64   \n",
      "1        12CA       A   1A23       A   0.9150    12178          6.28   \n",
      "2        12CA       A   1A43       A   0.8120     4097          0.00   \n",
      "3        12CA       A   1A5E       A   0.9850    11157          2.44   \n",
      "4        12CA       A   1A7V       A   0.8850     7372          1.77   \n",
      "...       ...     ...    ...     ...      ...      ...           ...   \n",
      "139489   8TIM       A   5ZYR       A   0.4030    23429          4.15   \n",
      "139490   8TIM       A   6BQG       A   0.2410    37374          6.19   \n",
      "139491   8TIM       A   6G4B       A   0.2990    35907          5.88   \n",
      "139492   8TIM       A   6JHM       A   0.5190    43159          3.34   \n",
      "139493   8TIM       A   6TQ3       A   0.0585    19171          4.71   \n",
      "\n",
      "        Similarity (%)  \n",
      "0                 9.09  \n",
      "1                12.57  \n",
      "2                 0.00  \n",
      "3                 7.32  \n",
      "4                 8.85  \n",
      "...                ...  \n",
      "139489            9.81  \n",
      "139490           16.81  \n",
      "139491           16.08  \n",
      "139492            7.90  \n",
      "139493            9.80  \n",
      "\n",
      "[139494 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to parse a line with PDB codes and chains\n",
    "def parse_pdb_line(line):\n",
    "    parts = line.split()\n",
    "    try:\n",
    "        code_1, code_2 = parts[1], parts[4]\n",
    "        chain_1, chain_2 = 'Unknown', 'Unknown'  # Default values\n",
    "        if '_' in code_1:\n",
    "            chain_1 = code_1.split('_')[1].replace('.pdb', '')\n",
    "            code_1 = code_1.split('_')[0]\n",
    "        if '_' in code_2:\n",
    "            chain_2 = code_2.split('_')[1].replace('.pdb', '')\n",
    "            code_2 = code_2.split('_')[0]\n",
    "        return code_1, chain_1, code_2, chain_2\n",
    "    except IndexError as e:\n",
    "        print(f\"Error processing line: {line}\")\n",
    "        raise e\n",
    "\n",
    "# Function to extract values from the line with P-value, Afp-num, etc.\n",
    "def parse_values_line(line):\n",
    "    p_value = float(re.search(r'P-value (\\S+)', line).group(1))\n",
    "    afp_num = int(re.search(r'Afp-num (\\d+)', line).group(1))\n",
    "    identity = float(re.search(r'Identity (\\S+%)', line).group(1).strip('%'))\n",
    "    similarity = float(re.search(r'Similarity (\\S+%)', line).group(1).strip('%'))\n",
    "    return p_value, afp_num, identity, similarity\n",
    "\n",
    "# Read the file and process it\n",
    "data = []\n",
    "with open('../data/homology/structural_homology.aln', 'r') as file:\n",
    "    for line in file:\n",
    "        if line.startswith('Align'):\n",
    "            #print(line)\n",
    "            code_1, chain_1, code_2, chain_2 = parse_pdb_line(line)\n",
    "        if 'P-value' in line:\n",
    "            p_value, afp_num, identity, similarity = parse_values_line(line)\n",
    "            data.append([code_1, chain_1, code_2, chain_2, p_value, afp_num, identity, similarity])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=['code_1', 'chain_1', 'code_2', 'chain_2', 'P-value', 'Afp-num', 'Identity (%)', 'Similarity (%)'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which datasets have identical mutants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/72928285.py:2: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  fireprot = list(pd.read_csv('../data/preprocessed/fireprot_mapped.csv')['code'].unique())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_1</th>\n",
       "      <th>chain_1</th>\n",
       "      <th>code_2</th>\n",
       "      <th>chain_2</th>\n",
       "      <th>P-value</th>\n",
       "      <th>Afp-num</th>\n",
       "      <th>Identity (%)</th>\n",
       "      <th>Similarity (%)</th>\n",
       "      <th>datasets_1</th>\n",
       "      <th>datasets_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12CA_A</td>\n",
       "      <td>A</td>\n",
       "      <td>1A0F_A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.9350</td>\n",
       "      <td>11638</td>\n",
       "      <td>3.64</td>\n",
       "      <td>9.09</td>\n",
       "      <td>['korpm']</td>\n",
       "      <td>['s461', 's669']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12CA_A</td>\n",
       "      <td>A</td>\n",
       "      <td>1A23_A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.9150</td>\n",
       "      <td>12178</td>\n",
       "      <td>6.28</td>\n",
       "      <td>12.57</td>\n",
       "      <td>['korpm']</td>\n",
       "      <td>['fireprot', 'q3421', 'korpm']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12CA_A</td>\n",
       "      <td>A</td>\n",
       "      <td>1A43_A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.8120</td>\n",
       "      <td>4097</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>['korpm']</td>\n",
       "      <td>['fireprot', 'q3421']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12CA_A</td>\n",
       "      <td>A</td>\n",
       "      <td>1A5E_A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>11157</td>\n",
       "      <td>2.44</td>\n",
       "      <td>7.32</td>\n",
       "      <td>['korpm']</td>\n",
       "      <td>['fireprot', 'q3421', 'korpm']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12CA_A</td>\n",
       "      <td>A</td>\n",
       "      <td>1A7V_A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.8850</td>\n",
       "      <td>7372</td>\n",
       "      <td>1.77</td>\n",
       "      <td>8.85</td>\n",
       "      <td>['korpm']</td>\n",
       "      <td>['s669', 'korpm']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139489</th>\n",
       "      <td>8TIM_A</td>\n",
       "      <td>A</td>\n",
       "      <td>5ZYR_A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.4030</td>\n",
       "      <td>23429</td>\n",
       "      <td>4.15</td>\n",
       "      <td>9.81</td>\n",
       "      <td>['fireprot']</td>\n",
       "      <td>['fireprot']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139490</th>\n",
       "      <td>8TIM_A</td>\n",
       "      <td>A</td>\n",
       "      <td>6BQG_A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>37374</td>\n",
       "      <td>6.19</td>\n",
       "      <td>16.81</td>\n",
       "      <td>['fireprot']</td>\n",
       "      <td>['fireprot']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139491</th>\n",
       "      <td>8TIM_A</td>\n",
       "      <td>A</td>\n",
       "      <td>6G4B_A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.2990</td>\n",
       "      <td>35907</td>\n",
       "      <td>5.88</td>\n",
       "      <td>16.08</td>\n",
       "      <td>['fireprot']</td>\n",
       "      <td>['fireprot']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139492</th>\n",
       "      <td>8TIM_A</td>\n",
       "      <td>A</td>\n",
       "      <td>6JHM_A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.5190</td>\n",
       "      <td>43159</td>\n",
       "      <td>3.34</td>\n",
       "      <td>7.90</td>\n",
       "      <td>['fireprot']</td>\n",
       "      <td>['fireprot']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139493</th>\n",
       "      <td>8TIM_A</td>\n",
       "      <td>A</td>\n",
       "      <td>6TQ3_A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>19171</td>\n",
       "      <td>4.71</td>\n",
       "      <td>9.80</td>\n",
       "      <td>['fireprot']</td>\n",
       "      <td>['fireprot']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139494 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        code_1 chain_1  code_2 chain_2  P-value  Afp-num  Identity (%)  \\\n",
       "0       12CA_A       A  1A0F_A       A   0.9350    11638          3.64   \n",
       "1       12CA_A       A  1A23_A       A   0.9150    12178          6.28   \n",
       "2       12CA_A       A  1A43_A       A   0.8120     4097          0.00   \n",
       "3       12CA_A       A  1A5E_A       A   0.9850    11157          2.44   \n",
       "4       12CA_A       A  1A7V_A       A   0.8850     7372          1.77   \n",
       "...        ...     ...     ...     ...      ...      ...           ...   \n",
       "139489  8TIM_A       A  5ZYR_A       A   0.4030    23429          4.15   \n",
       "139490  8TIM_A       A  6BQG_A       A   0.2410    37374          6.19   \n",
       "139491  8TIM_A       A  6G4B_A       A   0.2990    35907          5.88   \n",
       "139492  8TIM_A       A  6JHM_A       A   0.5190    43159          3.34   \n",
       "139493  8TIM_A       A  6TQ3_A       A   0.0585    19171          4.71   \n",
       "\n",
       "        Similarity (%)    datasets_1                      datasets_2  \n",
       "0                 9.09     ['korpm']                ['s461', 's669']  \n",
       "1                12.57     ['korpm']  ['fireprot', 'q3421', 'korpm']  \n",
       "2                 0.00     ['korpm']           ['fireprot', 'q3421']  \n",
       "3                 7.32     ['korpm']  ['fireprot', 'q3421', 'korpm']  \n",
       "4                 8.85     ['korpm']               ['s669', 'korpm']  \n",
       "...                ...           ...                             ...  \n",
       "139489            9.81  ['fireprot']                    ['fireprot']  \n",
       "139490           16.81  ['fireprot']                    ['fireprot']  \n",
       "139491           16.08  ['fireprot']                    ['fireprot']  \n",
       "139492            7.90  ['fireprot']                    ['fireprot']  \n",
       "139493            9.80  ['fireprot']                    ['fireprot']  \n",
       "\n",
       "[139494 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the unique structures from each dataset\n",
    "fireprot = list(pd.read_csv('../data/preprocessed/fireprot_mapped.csv')['code'].unique())\n",
    "s461 = list(pd.read_csv('../data/preprocessed/s461_mapped.csv')['code'].unique())\n",
    "s669 = list(pd.read_csv('../data/preprocessed/s669_mapped.csv')['code'].unique())\n",
    "q3421 = list(pd.read_csv('../data/preprocessed/q3421_mapped.csv')['code'].unique())\n",
    "ssym = list(pd.read_csv('../data/preprocessed/ssym_mapped.csv')['code'].unique())\n",
    "korpm_reduced = list(pd.read_csv('../data/preprocessed/korpm_mapped.csv')['code'].unique())\n",
    "korpm_full = list(pd.read_csv('../data/preprocessed/korpm_mapped.csv')['code'].unique())\n",
    "\n",
    "datasets = ['fireprot', 's461', 's669', 'q3421', 'ssym', 'korpm'] #'s669', \n",
    "df['datasets_1'] = [[] for _ in range(len(df))]\n",
    "df['datasets_2'] = [[] for _ in range(len(df))]\n",
    "\n",
    "# Iterate over each dataset and update the DataFrame\n",
    "for name, codes in zip(datasets, [fireprot, s461, s669, q3421, ssym, korpm_reduced]): #s669,\n",
    "    for i in df.index:\n",
    "        if df.at[i, 'code_1'] in codes:\n",
    "            df.at[i, 'datasets_1'].append(name)\n",
    "        if df.at[i, 'code_2'] in codes:\n",
    "            df.at[i, 'datasets_2'].append(name)\n",
    "\n",
    "df['datasets_1'] = df['datasets_1'].astype(str)\n",
    "df['datasets_2'] = df['datasets_2'].astype(str)\n",
    "df = df.loc[(df['datasets_1'].astype(str)!='[]') & (df['datasets_2'].astype(str)!='[]')]\n",
    "#df.sort_values('Similarity (%)', ascending=False).head(50)\n",
    "\n",
    "df['code_1'] = df['code_1'] + '_' + df['chain_1']\n",
    "df['code_2'] = df['code_2'] + '_' + df['chain_2']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which datasets have mutants with significant structural homology, forming clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fireprot protein clusters based on similarity:\n",
      "124\n",
      "[['1HZ6_A', '1RIS_A', '1APS_A', '2HPR_A'], ['1TTG_A', '1TEN_A', '1BOY_A', '1HNG_A'], ['1AYF_A', '1AAR_A', '1UBQ_A', '1AYF_B', '1OTR_B', '1FXA_A'], ['2ADA_A', '1WQ5_A', '2WSY_A'], ['1HK0_X'], ['1AZP_A', '1C8C_A', '1SSO_A'], ['1C52_A', '1YEA_A', '1YCC_A', '451C_A', '1YNR_A', '1AKK_A'], ['2CRK_A'], ['1MJC_A', '1C9O_A', '1CSP_A'], ['1I4N_A', '1HTI_A', '1KFW_A', '8TIM_A'], ['1LVE_A', '1TIT_A', '2IMM_A', '1WIT_A'], ['1CAH_A'], ['1G6N_A', '1AJ3_A'], ['1CLW_A'], ['6JHM_A', '6BQG_A'], ['1DKG_A', '2BRD_A', '1BVC_A', '1ROP_A', '3HHR_A', '1RHG_A', '2Q98_A', '1LS4_A', '1AV1_A'], ['1FTG_A', '2DRI_A'], ['1FC1_A'], ['1URK_A', '1TPK_A'], ['1E21_A', '1RTB_A', '1ONC_A'], ['1MSI_A'], ['1IOB_A', '2AFG_A', '3PG0_A', '1BFG_A'], ['1DIL_A'], ['5ZYR_A'], ['1RGG_A', '1MGR_A', '1BNI_A', '1RN1_C'], ['1H8V_A', '1BCX_A', '3WP4_A', '1OLR_A'], ['3PGK_A'], ['3BLS_A', '1BLC_A', '1AXB_A', '4BLM_A'], ['1A5E_A', '1IHB_A', '1SVX_A'], ['1B26_A', '1BVU_A', '1LBI_A'], ['1FEP_A'], ['1UWO_A', '1RTP_1', '1RRO_A', '1IGV_A'], ['2LZM_A', '1YYX_A'], ['1OIA_A'], ['4E5K_A'], ['1FMK_A', '1IR3_A'], ['1CYO_A', '1B5M_A', '1IET_A'], ['1STN_A'], ['1CTS_A'], ['1AMQ_A', '6G4B_A', '1H0C_A'], ['1BTA_A'], ['1HUE_A', '2TRT_A'], ['1CYC_A', '1C2R_A'], ['1ARR_A', '1P2P_A', '1BP2_A'], ['1PGA_A', '2ZTA_A'], ['1RX4_A'], ['1AON_O'], ['5AZU_A'], ['1TUP_A'], ['1QGV_A', '1H7M_A', '2TRX_A'], ['1ZYM_A'], ['2TS1_A'], ['1DPM_A', '1XAS_A', '1TUX_A', '2O9P_A'], ['1QGD_A'], ['2CBR_A', '1IFB_A', '1B0O_A', '1THQ_A', '1QJP_A', '1RBP_A'], ['1QLP_A', '1UHG_A', '1ANT_L', '1C5G_A'], ['2CI2_I', '1TIN_A', '1ACB_I'], ['1A23_A'], ['1TCA_A', '1MJ5_A', '3D2A_A', '1CQW_A'], ['1VQB_A'], ['1PX0_A', '2CHF_A', '6TQ3_A'], ['1BPI_A'], ['1HFY_A', '1LZ1_A', '1EL1_A', '4LYZ_A', '1HFZ_A'], ['1LRP_A', '1MBG_A'], ['4ZLU_A'], ['1BNL_A'], ['1QM4_A'], ['1YPI_A', '1ADO_A', '1TPE_A', '1BTM_A'], ['1DKT_A', '1SCE_A'], ['1A43_A', '1AG2_A'], ['1OH0_A'], ['1QQV_A'], ['1DEC_A'], ['1IMQ_A'], ['2UXY_A'], ['1GUY_C'], ['1JU3_A', '1EVQ_A'], ['1N0J_A', '1IDS_A'], ['1JNX_X'], ['1HYN_P'], ['1CHK_A'], ['1PIN_A'], ['1SUP_A'], ['1SHF_A', '2A36_A'], ['3VUB_A'], ['2HIP_A'], ['1POH_A'], ['1FRD_A'], ['1ANK_A', '2AKY_A'], ['1K9Q_A'], ['2ABD_A'], ['1HME_A'], ['1UZC_A'], ['1AM7_A'], ['2CPP_A'], ['3SSI_A'], ['1LUC_A'], ['1JIW_I'], ['3MBP_A'], ['1TDJ_A'], ['1ZNJ_A'], ['1FKJ_A'], ['1QND_A'], ['5CRO_O'], ['1KCQ_A'], ['1ZNJ_B'], ['1AQH_A'], ['1IO2_A'], ['1CEY_A', '1PDO_A'], ['1M21_A'], ['1BRF_A', '1IRO_A'], ['1KA6_A'], ['3PG4_A'], ['2HMB_A'], ['2RN2_A'], ['1AYE_A'], ['1BYW_A'], ['1W4E_A'], ['1BAH_A'], ['1CF3_A'], ['3TGL_A'], ['1JK9_A'], ['1DIV_A'], ['1KEV_A']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/3439229960.py:43: DtypeWarning: Columns (109,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(f'../data/inference/{name}_mapped_preds.csv', index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s461 protein clusters based on similarity:\n",
      "40\n",
      "[['3MON_B', '2ZTA_A'], ['1R2Y_A'], ['2H3F_A'], ['3S4M_A', '1EKG_A'], ['1FT8_A'], ['1J8I_A'], ['2HBB_A', '1DIV_A'], ['3DV0_I'], ['1A0F_A', '1ITM_A'], ['4BUQ_A'], ['2C9Q_A'], ['2N7Z_A'], ['2WQG_A'], ['2M5S_A'], ['1GUA_B', '1FRD_A', '1FXA_A'], ['1XXN_A'], ['3D3B_A'], ['1BFM_A'], ['5OAQ_A', '3L15_A'], ['5JXB_A'], ['2PTL_A'], ['1N88_A'], ['1G3P_A'], ['1O6X_A'], ['4HE7_A'], ['3BN0_A'], ['1IOJ_A'], ['1BA3_A'], ['1LVM_A'], ['2ARF_A'], ['1L6H_A'], ['1IV7_A', '1IV9_A'], ['2NTE_A'], ['1NM1_A'], ['3C2I_A'], ['1JL9_A'], ['1H0X_A'], ['2LTB_A'], ['1BNL_A'], ['1JLV_A']]\n",
      "s669 protein clusters based on similarity:\n",
      "63\n",
      "[['3MON_B', '4N6V_2', '1IV7_A', '1IV9_A'], ['1DXX_A'], ['2JIE_A'], ['4YEF_A', '4YEE_A'], ['3S4M_A', '2ZTA_A', '1EKG_A'], ['1FT8_A'], ['1XWS_A', '1IR3_A'], ['3D2A_A', '1H0X_A'], ['1J8I_A'], ['1X0J_A', '2N7Z_A', '2OUO_A', '3S92_A', '4BJX_A', '2DVV_A'], ['1A0F_A', '1ITM_A'], ['4BUQ_A'], ['2C9Q_A', '2CLR_B'], ['1FH5_L', '1FH5_H'], ['2WQG_A'], ['2M5S_A'], ['1HCQ_A', '1GLU_A'], ['1D5G_A', '5JXB_A', '3K82_A'], ['1BFM_A', '1R6R_A'], ['5OAQ_A', '3L15_A'], ['2PTL_A', '2BJD_A'], ['2VY0_A'], ['4HE7_A'], ['1IOJ_A'], ['1BA3_A'], ['1LVM_A'], ['1PFL_A'], ['1XZO_A'], ['1L6H_A'], ['1NM1_A'], ['1JL9_A'], ['1FRD_A', '1FXA_A', '1GUA_B'], ['1A7V_A', '1JLV_A'], ['2LTB_A'], ['1N18_A', '3ECU_A', '1SPD_A'], ['3G1G_A', '3O39_A'], ['1OSI_A'], ['1R2Y_A'], ['2H3F_A', '3FIS_A'], ['1F8I_A'], ['4WAA_A'], ['2HBB_A', '1DIV_A'], ['3DV0_I'], ['1XXN_A'], ['3D3B_A', '1PRG_A'], ['2RPN_A'], ['2KS4_A', '1GWY_A'], ['2PR5_A'], ['1PRE_A'], ['1N88_A'], ['1G3P_A'], ['1O6X_A'], ['2MPC_A'], ['3BN0_A'], ['2ARF_A'], ['2KJ3_A'], ['1O1U_A'], ['2NTE_A'], ['3C2I_A'], ['5VP3_A'], ['1BNL_A'], ['2JUC_A'], ['3BCI_A']]\n",
      "q3421 protein clusters based on similarity:\n",
      "83\n",
      "[['2IMM_A', '1LVE_A', '1HNG_A', '1TEN_A', '1TIT_A'], ['1TTG_A', '1BOY_A', '1FNF_A'], ['1A43_A', '2ZTA_A'], ['1AYF_A', '1OTR_B', '1AAR_A', '1UBQ_A', '1FXA_A', '1AYF_B'], ['1RIS_A', '2HPR_A', '1OIA_A', '1APS_A'], ['1OH0_A', '1THQ_A', '1QJP_A'], ['2ADA_A', '1WQ5_A', '2WSY_A'], ['1HK0_X'], ['1AZP_A', '1SSO_A'], ['3D2A_A'], ['2CRK_A'], ['1MJC_A', '1C9O_A', '1CSP_A'], ['1YEA_A', '1C2R_A', '1YCC_A', '1I5T_A', '1CYC_A', '451C_A', '1AKK_A'], ['1QQV_A'], ['1CAH_A'], ['1G6N_A', '1AJ3_A'], ['1CLW_A'], ['1TPK_A'], ['1IMQ_A'], ['1IR3_A'], ['2BRD_A', '1N0J_A', '1ROP_A', '1RHG_A', '2Q98_A', '1LS4_A'], ['1LZ1_A', '1EL1_A', '4LYZ_A', '1HFY_A'], ['1FTG_A', '1FLV_A'], ['1RRO_A', '1UWO_A', '1IGV_A', '1RTP_1'], ['1MSI_A'], ['1W4H_A'], ['1IFB_A', '1B0O_A', '2IFB_A', '2HMB_A'], ['1IOB_A', '2AFG_A'], ['1DIL_A'], ['1RGG_A', '1RN1_C', '1BNI_A'], ['1CHK_A'], ['1MBG_A', '1ZNJ_B'], ['3PGK_A'], ['1IDS_A'], ['3BLS_A', '1BLC_A', '4BLM_A', '1AXB_A'], ['1A5E_A', '1IHB_A'], ['1HTI_A', '1KFW_A'], ['1PIN_A'], ['1SUP_A'], ['1SHF_A', '2A36_A', '1SHG_A'], ['1B26_A'], ['1FEP_A'], ['1BVC_A', '1AV1_A', '3HHR_A'], ['1RTB_A', '1ONC_A'], ['2LZM_A', '1L63_A'], ['1POH_A'], ['1FRD_A'], ['1CYO_A', '1B5M_A'], ['1STN_A'], ['1ANK_A', '2AKY_A'], ['1AMQ_A'], ['1BTA_A'], ['2ABD_A'], ['1HME_A', '1KDX_A'], ['1UZC_A'], ['1AM7_A'], ['1ARR_A', '5CRO_O'], ['3SSI_A'], ['3MBP_A'], ['1PGA_A'], ['1RX4_A'], ['1AON_O'], ['5AZU_A'], ['1H7M_A', '1QGV_A', '2TRX_A'], ['1ZNJ_A'], ['1FKJ_A'], ['1TUP_A'], ['1AG2_A'], ['1IO2_A'], ['1DPM_A'], ['1CEY_A'], ['2RN2_A'], ['1SCE_A', '1DKT_A'], ['1QLP_A', '1C5G_A'], ['2CI2_I'], ['1A23_A'], ['1DIV_A'], ['1VQB_A'], ['1IRO_A'], ['1BPI_A'], ['1BP2_A'], ['1SAK_A'], ['1BNL_A']]\n",
      "ssym protein clusters based on similarity:\n",
      "12\n",
      "[['1CEY_A'], ['2LZM_A', '1L63_A'], ['1OH0_A'], ['1LZ1_A', '4LYZ_A'], ['1RN1_C', '1BNI_A'], ['5PTI_A'], ['1VQB_A'], ['2RN2_A'], ['1EY0_A'], ['1AMQ_A'], ['1IHB_A'], ['1IOB_A']]\n",
      "korpm protein clusters based on similarity:\n",
      "97\n",
      "[['1TTG_A', '1K85_A', '1FNF_A'], ['1RIS_A', '1OIA_A', '1APS_A', '2K3K_A', '2K7J_A', '2BJD_A'], ['1HK0_X'], ['3S4M_A', '2IFB_A', '1IV9_A'], ['1AZP_A', '1BNZ_A', '1J8I_A'], ['1MJC_A', '1C9O_A', '1CSP_A'], ['1LVE_A', '2MMX_A', '1FH5_L', '1TIT_A', '1FH5_H', '2IMM_A', '1OPG_L'], ['1CAH_A', '12CA_A'], ['1AYI_A', '2SPZ_A', '1IMQ_A'], ['2LSB_A', '1RHG_A', '2Q98_A', '1LS4_A', '1QM0_A', '1LPE_A', '1AJ3_A'], ['1CFD_A', '1RRO_A', '2LCP_A', '5VP3_A'], ['2WQG_A'], ['1FTG_A', '2DRI_A'], ['1ITM_A', '1A7V_A'], ['1MSI_A'], ['1W4H_A', '3DV0_I'], ['2PTL_A', '1UBQ_A', '1PGA_A'], ['1IOB_A', '2AFG_A', '1K5U_A', '1JY0_A', '1JQZ_A'], ['1DIL_A'], ['1RGG_A', '1MGR_A', '1BNI_A', '1RN1_C'], ['2IN9_A'], ['1U4Q_A', '1BVC_A'], ['3PGK_A', '2WZB_A', '5NP8_A'], ['1PFL_A'], ['2JOF_A'], ['1A5E_A', '1NFI_F', '1IHB_A'], ['1L6H_A'], ['1IV7_A', '5T43_A'], ['1CM2_A', '1O6X_A', '2HPR_A', '1Y4Y_A', '1POH_A'], ['2LZM_A', '1L63_A'], ['1YCC_A', '1CYC_A', '451C_A', '1AKK_A', '1C2R_A', '1I5T_A'], ['1STN_A', '3BDC_A'], ['1N18_A'], ['2LTB_A'], ['1BTA_A'], ['1ARR_A', '3G1G_A'], ['1RX4_A'], ['3MXF_A'], ['2HBB_A', '1DIV_A'], ['1E0L_A', '2ZAJ_A', '4GWT_A'], ['1Z1I_A'], ['1B5M_A'], ['1WQ5_A'], ['2RPN_A', '1SHG_A', '2A36_A'], ['1QGV_A', '1H7M_A', '2TRX_A'], ['1N88_A'], ['1KFW_A'], ['2MPC_A'], ['2ARF_A'], ['3L15_A'], ['1QLP_A'], ['2CI2_I', '1EGL_A', '1ACB_I'], ['1AXB_A', '1BTL_A'], ['2JYS_A'], ['1FKB_A', '1FKJ_A'], ['1A23_A'], ['1B10_A', '1XYX_A', '1AG2_A'], ['5TR5_A'], ['1GWY_A', '2KS4_A'], ['1BPI_A'], ['1HFY_A', '1LZ1_A', '1EL1_A', '4LYZ_A', '1HFZ_A'], ['2JUC_A', '1MBG_A'], ['1DKT_A'], ['1FT8_A'], ['3D2A_A'], ['1QQV_A', '1VII_A'], ['1TPK_A'], ['1RIL_A', '1IO2_A'], ['1JNX_X', '2NTE_A'], ['1CHK_A'], ['1IOJ_A', '1HME_A'], ['1BA3_A'], ['2RN4_A'], ['1PIN_A'], ['1SUP_A'], ['1GLM_A'], ['1RTB_A', '1ONC_A'], ['1FLV_A'], ['1IGV_A'], ['2ABD_A'], ['1FXA_A'], ['1OSI_A'], ['1UZC_A'], ['1AM7_A'], ['2H3F_A'], ['1THQ_A'], ['3SSI_A'], ['2AKY_A'], ['3MBP_A'], ['1QND_A'], ['1XXN_A'], ['2PR5_A'], ['1PRE_A'], ['1G3P_A'], ['1CEY_A'], ['2RN2_A'], ['3C2I_A']]\n"
     ]
    }
   ],
   "source": [
    "### cluster based on E-value (structural)\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_cluster(protein, assigned_clusters, threshold=0.01):\n",
    "    for cluster in assigned_clusters:\n",
    "        if all(similarity_matrix.at[protein, member] <= threshold for member in cluster):\n",
    "            return cluster\n",
    "    return None\n",
    "\n",
    "for name, codes in zip(datasets, [fireprot, s461, s669, q3421, ssym, korpm_reduced]):\n",
    "    df_cur = df.copy(deep=True).loc[df['datasets_1'].astype(str).str.contains(name)]\n",
    "    df_cur = df_cur.loc[df['datasets_2'].astype(str).str.contains(name)]\n",
    "    #df_cur = df_cur.loc[df['Similarity (%)']>50]\n",
    "    # Create a list of all unique codes\n",
    "    all_codes = set(df_cur['code_1']).union(set(df_cur['code_2']))\n",
    "\n",
    "    # Pivot to create a similarity matrix\n",
    "    similarity_matrix = df_cur.pivot(index='code_1', columns='code_2', values='P-value')\n",
    "\n",
    "    # Reindex the DataFrame to include all codes in both rows and columns\n",
    "    similarity_matrix = similarity_matrix.reindex(index=all_codes, columns=all_codes)\n",
    "\n",
    "    # Fill NaN values with 0 and make the matrix symmetric\n",
    "    similarity_matrix = similarity_matrix.fillna(0)\n",
    "    similarity_matrix = similarity_matrix + similarity_matrix.T - similarity_matrix.multiply(similarity_matrix.T.gt(0))\n",
    "\n",
    "    # Assign proteins to clusters\n",
    "    clusters = defaultdict(list)\n",
    "    for protein in similarity_matrix.index:\n",
    "        cluster = find_cluster(protein, clusters.values())\n",
    "        if cluster is not None:\n",
    "            cluster.append(protein)\n",
    "        else:\n",
    "            clusters[len(clusters)].append(protein)\n",
    "\n",
    "    # Convert the clusters dictionary to a list for better readability\n",
    "    cluster_list = list(clusters.values())\n",
    "\n",
    "    print(name, \"protein clusters based on similarity:\")\n",
    "    print(len(cluster_list))\n",
    "    print(cluster_list)\n",
    "\n",
    "    data = pd.read_csv(f'../data/inference/{name}_mapped_preds.csv', index_col=0)\n",
    "    if name == 's461':\n",
    "        data['code'] = data.index.str[:4]\n",
    "        \n",
    "    data['cluster'] = 0\n",
    "    i = 0\n",
    "    for clus in cluster_list:\n",
    "        i += 1\n",
    "        for code_ in clus:\n",
    "            code = code_[:4]\n",
    "            chain = code_[-1]\n",
    "            data.loc[(data['code']==code)&(data['chain']==chain), 'cluster'] = i\n",
    "    data.to_csv(f'../data/inference/{name}_mapped_preds_clusters.csv')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save information for use in the dataset-specific analysis notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fireprot s461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/1832870676.py:6: DtypeWarning: Columns (109,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ds1 = pd.read_csv(f'../data/inference/{name1}_mapped_preds_clusters.csv', index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fireprot s669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/1832870676.py:6: DtypeWarning: Columns (109,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ds1 = pd.read_csv(f'../data/inference/{name1}_mapped_preds_clusters.csv', index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fireprot q3421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/1832870676.py:6: DtypeWarning: Columns (109,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ds1 = pd.read_csv(f'../data/inference/{name1}_mapped_preds_clusters.csv', index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fireprot ssym\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/1832870676.py:6: DtypeWarning: Columns (109,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ds1 = pd.read_csv(f'../data/inference/{name1}_mapped_preds_clusters.csv', index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fireprot korpm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/1832870676.py:6: DtypeWarning: Columns (109,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ds1 = pd.read_csv(f'../data/inference/{name1}_mapped_preds_clusters.csv', index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s461 fireprot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/1832870676.py:7: DtypeWarning: Columns (109,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ds2 = pd.read_csv(f'../data/inference/{name2}_mapped_preds_clusters.csv', index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s461 s669\n",
      "s461 q3421\n",
      "s461 ssym\n",
      "s461 korpm\n",
      "s669 fireprot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/1832870676.py:7: DtypeWarning: Columns (109,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ds2 = pd.read_csv(f'../data/inference/{name2}_mapped_preds_clusters.csv', index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s669 s461\n",
      "s669 q3421\n",
      "s669 ssym\n",
      "s669 korpm\n",
      "q3421 fireprot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/1832870676.py:7: DtypeWarning: Columns (109,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ds2 = pd.read_csv(f'../data/inference/{name2}_mapped_preds_clusters.csv', index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q3421 s461\n",
      "q3421 s669\n",
      "q3421 ssym\n",
      "q3421 korpm\n",
      "ssym fireprot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/1832870676.py:7: DtypeWarning: Columns (109,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ds2 = pd.read_csv(f'../data/inference/{name2}_mapped_preds_clusters.csv', index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssym s461\n",
      "ssym s669\n",
      "ssym q3421\n",
      "ssym korpm\n",
      "korpm fireprot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/1832870676.py:7: DtypeWarning: Columns (109,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ds2 = pd.read_csv(f'../data/inference/{name2}_mapped_preds_clusters.csv', index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korpm s461\n",
      "korpm s669\n",
      "korpm q3421\n",
      "korpm ssym\n"
     ]
    }
   ],
   "source": [
    "# detect sequence overlaps > 25% (for structurally defined region)\n",
    "for name1 in datasets:\n",
    "    for name2 in datasets:\n",
    "        if name1 != name2:\n",
    "            print(name1, name2)\n",
    "            ds1 = pd.read_csv(f'../data/inference/{name1}_mapped_preds_clusters.csv', index_col=0)\n",
    "            ds2 = pd.read_csv(f'../data/inference/{name2}_mapped_preds_clusters.csv', index_col=0)\n",
    "            overlap = df.loc[((df['datasets_1'].str.contains(name1)) & (df['datasets_2'].str.contains(name2)) | (df['datasets_1'].str.contains(name2)) & (df['datasets_2'].str.contains(name1))) & (df['Identity (%)']>25)]\n",
    "            overlapping_codes = list(overlap['code_1'].str[:4].unique()) + list(overlap['code_2'].str[:4].unique())\n",
    "            overlapping_codes += list(set(ds1['code'].unique()).intersection(set(ds2['code'].unique())))\n",
    "            ds1[f'{name2}_cluster'] = False\n",
    "            ds2[f'{name1}_cluster'] = False\n",
    "            ds1.loc[ds1['code'].isin(overlapping_codes), f'{name2}_cluster'] = True\n",
    "            ds2.loc[ds2['code'].isin(overlapping_codes), f'{name1}_cluster'] = True\n",
    "            ds1.to_csv(f'../data/inference/{name1}_mapped_preds_clusters.csv')\n",
    "            ds2.to_csv(f'../data/inference/{name2}_mapped_preds_clusters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect information from running MMSeqs2 (sequence-only clustering) (expected at ../data/homology/sequence_homology.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/2335251976.py:53: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(file2, index_col=0)\n",
      "/tmp/ipykernel_2178633/2335251976.py:53: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(file2, index_col=0)\n",
      "/tmp/ipykernel_2178633/2335251976.py:53: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(file2, index_col=0)\n",
      "/tmp/ipykernel_2178633/2335251976.py:33: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv(file1, index_col=0)\n",
      "/tmp/ipykernel_2178633/2335251976.py:53: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(file2, index_col=0)\n",
      "/tmp/ipykernel_2178633/2335251976.py:53: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(file2, index_col=0)\n",
      "/tmp/ipykernel_2178633/2335251976.py:53: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(file2, index_col=0)\n"
     ]
    }
   ],
   "source": [
    "# cluster at 25% sequence identity based on MMSeqs2\n",
    "\n",
    "data = pd.read_csv('../data/homology/sequence_homology.tsv', sep='\\t', header=None)\n",
    "data = data.iloc[:, :3]\n",
    "data.columns = ['source', 'target', 'identity']\n",
    "data = data.loc[data['identity']>0.25]\n",
    "data = data.loc[data['source']!=data['target']]\n",
    "all_codes = set(data['source']).union(set(data['target']))\n",
    "\n",
    "tmp = pd.read_csv(f'../data/inference/korpm_mapped_preds_clusters.csv', index_col=0)\n",
    "tmp = tmp[[c for c in tmp.columns if not 'overlaps' in c]]\n",
    "tmp.to_csv(f'../data/inference/korpm_mapped_preds_clusters.csv')\n",
    "\n",
    "tmp = pd.read_csv(f'../data/inference/q3421_mapped_preds_clusters.csv', index_col=0)\n",
    "tmp = tmp[[c for c in tmp.columns if not 'overlaps' in c]]\n",
    "tmp.to_csv(f'../data/inference/q3421_mapped_preds_clusters.csv')\n",
    "\n",
    "#tmp = pd.read_csv('../data/fireprot_mapped_preds_clusters.csv', index_col=0)\n",
    "#tmp = tmp[[c for c in tmp.columns if not 'overlaps' in c]]\n",
    "#tmp.to_csv('../data/fireprot_mapped_preds_clusters.csv')\n",
    "\n",
    "id_table = pd.DataFrame()\n",
    "homo_table = pd.DataFrame()\n",
    "\n",
    "for file1 in ['../data/preprocessed/korpm_mapped.csv', \n",
    "              '../data/external_datasets/rosetta_mapped.csv', \n",
    "              '../data/preprocessed/tsuboyama_mapped.csv', \n",
    "              '../data/preprocessed/fireprot_mapped.csv', \n",
    "              '../data/preprocessed/q3421_mapped.csv',\n",
    "              '../data/preprocessed/s461_mapped.csv',\n",
    "              '../data/preprocessed/ssym_mapped.csv'\n",
    "              ]:  \n",
    "    df_train = pd.read_csv(file1, index_col=0)\n",
    "    if 'fireprot_mapped.csv' in file1:\n",
    "        df_train['position'] = df_train['position'].fillna(-100000).astype(int)\n",
    "        df_train['uid2'] = df_train['code'] + '_' + df_train['position'].astype(str) + df_train['mutation']\n",
    "        df_train = df_train.reset_index()\n",
    "        df_train = df_train.groupby('uid2').first()\n",
    "    train_codes = set(df_train['code'])\n",
    "    name1 = file1.split('/')[-1].split('_')[0]\n",
    "    for file2 in ['../data/preprocessed/korpm_mapped.csv', \n",
    "                '../data/external_datasets/rosetta_mapped.csv', \n",
    "                '../data/preprocessed/tsuboyama_mapped.csv', \n",
    "                '../data/preprocessed/fireprot_mapped.csv', \n",
    "                '../data/preprocessed/q3421_mapped.csv',\n",
    "                '../data/preprocessed/s461_mapped.csv',\n",
    "                '../data/preprocessed/ssym_mapped.csv'\n",
    "                ]:  \n",
    "        c = 'code' if ('ssym' not in file2) else 'wt_code'\n",
    "        name2 = file2.split('/')[-1].split('_')[0]\n",
    "        if file1 != file2:\n",
    "            overlap = set()\n",
    "            df_test = pd.read_csv(file2, index_col=0)\n",
    "            if 'fireprot_mapped.csv' in file2:\n",
    "                df_test['position'] = df_test['position'].fillna(-100000).astype(int)\n",
    "                df_test['uid2'] = df_test['code'] + '_' + df_test['position'].astype(str) + df_test['mutation']\n",
    "                df_test = df_test.reset_index()\n",
    "                df_test = df_test.groupby('uid2').first()\n",
    "            #print(len(df_train), len(df_test))\n",
    "            id_table.at[name1, name2] = len(df_train.join(df_test[[]], how='inner'))\n",
    "            test_codes = set(df_test['code'])\n",
    "            #cc_test = df_test.loc[df_test['code'].isin(overlap_codes)]\n",
    "            for code in train_codes:\n",
    "                if code in test_codes:\n",
    "                    overlap.add(code)\n",
    "                else:\n",
    "                    odf = data.loc[(data['source']==code)|(data['target']==code)]\n",
    "                    odf = odf.loc[data['source'].isin(test_codes)|data['target'].isin(test_codes)]\n",
    "                    #print(odf)\n",
    "                    if len(odf) > 0:\n",
    "                        overlap.add(code)\n",
    "            #print(name1, name2, overlap)\n",
    "            homo_table.at[name1, name2] = len(df_test.loc[df_test[c].isin(overlap)])\n",
    "            #print(len(df_test.loc[df_test['code'].isin(overlap)]))\n",
    "\n",
    "            if 'q3421' in name1:\n",
    "                name2_ = name2\n",
    "                df_train[f'overlaps_{name2_}'] = False\n",
    "                df_train.loc[df_train['code'].isin(overlap), f'overlaps_{name2_}'] = True\n",
    "                tmp = pd.read_csv('../data/inference/q3421_mapped_preds_clusters.csv', index_col=0)\n",
    "                tmp = tmp.join(df_train[[f'overlaps_{name2_}']])\n",
    "                tmp.to_csv('../data/inference/q3421_mapped_preds_clusters.csv')\n",
    "\n",
    "            if 'korpm' in name1:\n",
    "                name2_ = name2\n",
    "                df_train[f'overlaps_{name2_}'] = False\n",
    "                df_train.loc[df_train['code'].isin(overlap), f'overlaps_{name2_}'] = True\n",
    "                tmp = pd.read_csv('../data/inference/korpm_mapped_preds_clusters.csv', index_col=0)\n",
    "                tmp = tmp.join(df_train[[f'overlaps_{name2_}']])\n",
    "                tmp.to_csv('../data/inference/korpm_mapped_preds_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s461\n",
      "669 461\n",
      "ssym\n",
      "684 684\n",
      "Index(['esmif_monomer_dir', 'runtime_esmif_monomer_dir',\n",
      "       'pll_esmif_monomer_dir', 'Cartddg_dir', 'Evo_dir', 'KORPM_dir',\n",
      "       'ddG_dir', 'esm2_15B_half_dir', 'esm2_3B_dir', 'mif_dir',\n",
      "       ...\n",
      "       'INPS3D_dir', 'INPS3D_inv', 'INPS-Seq_dir', 'INPS-Seq_inv',\n",
      "       'ThermoNet_dir', 'ThermoNet_inv', 'Dynamut2_dir', 'Dynamut2_inv',\n",
      "       'cluster_dir', 'cluster_inv'],\n",
      "      dtype='object', length=190)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/1536594660.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  db2.loc[:, ['uid']] = db2['wt_code'] + '_' + db2['position_orig'].astype(str) + db2['wild_type']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korpm\n",
      "2369 2371\n",
      "q3421\n",
      "3421 3421\n",
      "fireprot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2178633/1536594660.py:16: DtypeWarning: Columns (109,110,111) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  db = pd.read_csv(file1).set_index(['uid', 'uid2'])\n"
     ]
    }
   ],
   "source": [
    "# open each of the main dataset inference files\n",
    "for file1 in ['../data/inference/s461_mapped_preds_clusters.csv',\n",
    "              '../data/inference/ssym_mapped_preds_clusters.csv',\n",
    "              '../data/inference/korpm_mapped_preds_clusters.csv', \n",
    "              '../data/inference/q3421_mapped_preds_clusters.csv',\n",
    "              '../data/inference/fireprot_mapped_preds_clusters.csv']:  \n",
    "\n",
    "    dataset = file1.split('/')[-1].split('_')[0]\n",
    "    dataset_ = dataset\n",
    "\n",
    "    print(dataset)\n",
    "    if dataset == 's461':\n",
    "    # since s461 is a subset of s669, can just use calcs for s669\n",
    "        dataset_ = 's669'\n",
    " \n",
    "    db = pd.read_csv(file1).set_index(['uid', 'uid2'])\n",
    "    \n",
    "    # load effective number of sequences from separate file (generated by MSA transformer)\n",
    "    neff = pd.read_csv(os.path.join('..', 'data', 'features', f'neff_{dataset_}.csv'), header=None, index_col=0)\n",
    "    neff.index.name = 'code'\n",
    "    neff.columns = ['neff', 'sequence_length']\n",
    "\n",
    "    # neff file was generated with different sized alignments, the largest in terms of Neff was used\n",
    "    neff = neff.groupby(level=0).max()\n",
    "\n",
    "    db_feats = pd.read_csv(os.path.join('..', 'data', 'features', f'{dataset_}_local_mapped_feats.csv'))\n",
    "    db_feats['uid'] = db_feats['code'] + '_' + db_feats['position_orig'].astype(str) + db_feats['mutation']\n",
    "    db_feats['uid2'] = db_feats['code'] + '_' + db_feats['position'].fillna(-1000000).astype(int).astype(str) + db_feats['mutation']\n",
    "\n",
    "    db_feats = db_feats.set_index(['uid', 'uid2'])\n",
    "    db_feats = db_feats[['on_interface', 'entropy', 'conservation', 'column_completeness', 'completeness_score', 'n_seqs', 'structure_length', 'SS', 'code',\n",
    "                         'features', 'hbonds', 'saltbrs', 'b_factor', 'kdh_wt', 'kdh_mut', 'vol_wt', 'vol_mut', 'chg_wt', 'chg_mut', 'rel_ASA']] #'residue_depth', 'wt_code',\n",
    "\n",
    "    db_feats['on_interface'] = db_feats['on_interface'].astype(int)\n",
    "    db_feats['features'] = db_feats['features'].fillna(\"\")\n",
    "    db_feats['delta_kdh'] = db_feats['kdh_mut'] - db_feats['kdh_wt']\n",
    "    db_feats['delta_vol'] = db_feats['vol_mut'] - db_feats['vol_wt']\n",
    "    db_feats['delta_chg'] = db_feats['chg_mut'] - db_feats['chg_wt']\n",
    "    db_feats['to_proline'] = (db_feats.reset_index('uid2').index.str[-1] == 'P').astype(int)\n",
    "    db_feats['to_glycine'] = (db_feats.reset_index('uid2').index.str[-1] == 'G').astype(int)\n",
    "    db_feats['from_proline'] = (db_feats.reset_index('uid2').index.str[6] == 'P').astype(int)\n",
    "    db_feats['from_glycine'] = (db_feats.reset_index('uid2').index.str[6] == 'G').astype(int)\n",
    "    db_feats['helix'] = db_feats['SS'] == 'H'\n",
    "    db_feats['bend'] = db_feats['SS'] == 'S'\n",
    "    db_feats['turn'] = db_feats['SS'] == 'T'\n",
    "    db_feats['coil'] = db_feats['SS'] == '-'\n",
    "    db_feats['strand'] = db_feats['SS'] == 'E'\n",
    "    db_feats['active_site'] = db_feats['features'].str.contains('ACT_SITE')\n",
    "\n",
    "    db_feats = db_feats.drop(['kdh_wt', 'kdh_mut', 'vol_wt', 'vol_mut', 'chg_wt', 'chg_mut', 'features', 'SS'], axis=1)\n",
    "    db_feats = db_feats.reset_index().merge(neff['neff'].dropna(), on='code', how='left').drop('code', axis=1).set_index(['uid', 'uid2'])\n",
    "    db_feats['neff'] = db_feats['neff'].fillna(0)\n",
    "    #unique_indices = db_feats.groupby('uid')['neff'].idxmax()#.astype(int)\n",
    "    #db_feats = db_feats.loc[unique_indices].set_index(['uid', 'uid2'])\n",
    "\n",
    "    for feature in ['on_interface', 'features', 'rel_ASA', 'delta_kdh', 'delta_vol', 'delta_chg', 'to_proline', 'to_glycine', 'from_proline', 'from_glycine', 'helix', 'bend', 'turn', 'coil', 'strand', 'active_site']:\n",
    "        db_feats = db_feats.rename({feature: feature + '_dir'}, axis=1)\n",
    "\n",
    "    len_db = len(db)\n",
    "\n",
    "    if dataset != 'fireprot':\n",
    "        print(len(db_feats), len_db)\n",
    "        #assert len(db_feats) == len_db\n",
    "        db_mod = db.join(db_feats, how='left')\n",
    "    else:\n",
    "        db_feats = db_feats.drop(['b_factor', 'conservation'], axis=1)\n",
    "        db_mod = db.join(db_feats.rename({'uid': 'uid_'}, axis=1).rename({'uid2': 'uid'}, axis=1).rename({'uid_': 'uid2'}, axis=1), how='left')\n",
    "\n",
    "    if dataset == 'ssym':\n",
    "        # apply the structural clusters assigned to wild-type structures to mutants\n",
    "        for code in db_mod['wt_code'].unique():\n",
    "            cluster = db_mod.loc[db_mod['code']==code, 'cluster'].head(1).item()\n",
    "            db_mod.loc[db_mod['wt_code']==code, 'cluster'] = cluster\n",
    "\n",
    "        # assign a new direction column to keep track of wild type vs mutant structures\n",
    "        db_mod['direction'] = 'dir'\n",
    "        db_mod.loc[db_mod['code']!=db_mod['wt_code'], 'direction'] = 'inv'\n",
    "\n",
    "        # match the naming convention for predictions made by other authors\n",
    "        for col in ['KORPM', 'Cartddg', 'FoldX', 'Evo', 'Dyna2', 'PopMs', 'DDGun', 'TNet', 'ACDCNN', 'ddG', 'cluster']:\n",
    "            db_mod = db_mod.rename({col: col + '_dir'}, axis=1)\n",
    "\n",
    "        # get two new dataframes which are just the forward and reverse mutations, and then hstack them\n",
    "        db1 = db_mod.loc[db_mod['code'].str[:4]==db_mod['wt_code']]\n",
    "        db1 = db1.drop(['code', 'wt_code'], axis=1)\n",
    "        db2 = db_mod.loc[db_mod['code'].str[:4]!=db_mod['wt_code']]\n",
    "        db2.loc[:, ['uid']] = db2['wt_code'] + '_' + db2['position_orig'].astype(str) + db2['wild_type']\n",
    "        db2 = db2.set_index('uid')\n",
    "        db2 = db2.drop(['code', 'wt_code'], axis=1)\n",
    "        db2.columns = [c.replace('_dir', '_inv') for c in db2.columns]\n",
    "        db2 = db2[[c for c in db2.columns if '_inv' in c]]\n",
    "        db_flat = db1.join(db2)\n",
    "\n",
    "        # sequence methods are necessarily antisymmetric. This fills in missing or erroneous values\n",
    "        for col in db_flat.columns:\n",
    "            if '_dir' in col:\n",
    "                if any([e in col for e in ['esm2', 'esm1v', 'msa', 'tranception', 'ankh']]) and not 'runtime' in col:\n",
    "                    db_flat[col.replace('_dir', '_inv')] = -db_flat[col]\n",
    "\n",
    "        #db_ddgs_2 = db_flat[['ddG_dir', 'ddG_inv']]\n",
    "\n",
    "        # merge with Ssym+\n",
    "        ssymp = pd.read_csv(os.path.join('..', 'data', 'external_datasets', 'Ssym+_experimental.csv'))\n",
    "        ssymp['uid'] = ssymp['Protein'].str[:4].apply(lambda x: x.upper())  + '_' + ssymp['Mut_pdb'].str[1:]\n",
    "        ssymp = ssymp.set_index('uid')\n",
    "\n",
    "        test = db_flat.reset_index().set_index('uid').join(ssymp, lsuffix='_plus').reset_index().set_index('uid')\n",
    "\n",
    "        float_columns = list(test.select_dtypes(include=['float']).columns)\n",
    "        float_columns.extend(['cluster_dir', 'cluster_inv'])\n",
    "        db_class = test[float_columns]\n",
    "        db_class.columns = ['plus_' + c[:-5] if 'plus' in c else c for c in db_class.columns]\n",
    "        db_class = db_class.drop([c+'_dir' for c in ['ACDCNN', 'ACDC-NN-2str', 'plus_FoldX', 'plus_DDGun', 'PopMs', 'TNet', 'Dyna2']], axis=1)\n",
    "        db_class = db_class.drop([c+'_inv' for c in ['ACDCNN', 'ACDC-NN-2str', 'plus_FoldX', 'plus_DDGun', 'PopMs', 'TNet', 'Dyna2']], axis=1)\n",
    "        print(db_class.columns)\n",
    "        db_stacked = analysis_utils.stack_frames(db_class)\n",
    "\n",
    "        cols = db_mod.columns\n",
    "        cols = [c.replace('_dir', '') for c in cols]\n",
    "        db_mod.columns = cols\n",
    "        \n",
    "        join_cols = [c for c in db_mod.columns if not c in db_stacked.columns]\n",
    "        join_cols.remove('direction')\n",
    "        db_mod = db_mod.reset_index(drop=True)\n",
    "        db_mod['uid'] = db_mod['wt_code'] + '_' + db_mod['position_orig'].astype(str) + db_mod['mutation']\n",
    "\n",
    "        db_stacked = db_stacked.reset_index('direction')\n",
    "        db_stacked = db_stacked.join(db_mod.set_index('uid')[join_cols])\n",
    "        \n",
    "        db_stacked = db_stacked.reset_index()\n",
    "        db_stacked['uid2'] = db_stacked['wt_code'] + '_' + db_stacked['position'].astype(str) + db_stacked['mutation']\n",
    "        db_stacked = db_stacked.set_index(['direction', 'uid', 'uid2'])\n",
    "\n",
    "        #out_loc_flat = f'../data/analysis/{dataset}_flat_analysis.csv'\n",
    "        #db_stacked.to_csv(out_loc_flat)\n",
    "        db_mod = db_stacked\n",
    "\n",
    "    elif dataset == 's461':\n",
    "        # create and use a third index for matching with the S461 subset\n",
    "        db_full = db_mod.copy(deep=True)\n",
    "        db_full['uid3'] = db['code'] + '_' + db['PDB_Mut'].str[1:]\n",
    "        db_full = db_full.reset_index().set_index('uid3')\n",
    "\n",
    "        # preprocess S461 to align with S669\n",
    "        s461 = pd.read_csv(os.path.join('..', 'data', 'external_datasets', 'S461.csv'))\n",
    "        s461['uid3'] = s461['PDB'] + '_' + s461['MUT_D'].str[2:]\n",
    "        s461 = s461.set_index('uid3')\n",
    "        s461['ddG_I'] = -s461['ddG_D']\n",
    "        s461.columns = [s+'_dir' for s in s461.columns]\n",
    "        s461 = s461.rename({'ddG_D_dir': 'ddG_dir', 'ddG_I_dir': 'ddG_inv'}, axis=1)\n",
    "\n",
    "        # merge S669 with S461 (keeping predictions from both for comparison purposes)\n",
    "        db_mod = s461.join(db_full.drop(['PDB_dir', 'MUT_D_dir', 'ddG_dir', 'KORPMD_dir', 'CartddgD_dir',\n",
    "            'FoldXD_dir', 'EvoD_dir', 'Dyna2D_dir', 'PopMsD_dir', 'DDGunD_dir',\n",
    "            'TNetD_dir', 'ACDCNND_dir', 'ddG_inv'], axis=1), how='left').reset_index(drop=True).set_index(['uid', 'uid2'])\n",
    "\n",
    "    if 'ddG' in db_mod.columns and not 'ddG_dir' in db_mod.columns:\n",
    "        db_mod['ddG_dir'] = db_mod['ddG']\n",
    "    elif 'ddG_dir' in db_mod.columns and not 'ddG' in db_mod.columns:\n",
    "        db_mod['ddG'] = db_mod['ddG_dir']\n",
    "        \n",
    "    out_loc = f'../data/analysis/{dataset}_analysis.csv'\n",
    "    db_mod.to_csv(out_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rosetta</th>\n",
       "      <th>tsuboyama</th>\n",
       "      <th>fireprot</th>\n",
       "      <th>q3421</th>\n",
       "      <th>s461</th>\n",
       "      <th>ssym</th>\n",
       "      <th>korpm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>korpm</th>\n",
       "      <td>299.0</td>\n",
       "      <td>860.0</td>\n",
       "      <td>1394.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rosetta</th>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>299.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsuboyama</th>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>860.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fireprot</th>\n",
       "      <td>619.0</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2244.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q3421</th>\n",
       "      <td>908.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2244.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s461</th>\n",
       "      <td>4.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssym</th>\n",
       "      <td>124.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rosetta  tsuboyama  fireprot   q3421   s461   ssym   korpm\n",
       "korpm        299.0      860.0    1394.0  1050.0  141.0    0.0     NaN\n",
       "rosetta        NaN       33.0     619.0   908.0    4.0  124.0   299.0\n",
       "tsuboyama     33.0        NaN    1022.0   106.0  135.0    0.0   860.0\n",
       "fireprot     619.0     1022.0       NaN  2244.0   49.0  170.0  1394.0\n",
       "q3421        908.0      106.0    2244.0     NaN   50.0  205.0  1050.0\n",
       "s461           4.0      135.0      49.0    50.0    NaN    0.0   141.0\n",
       "ssym         124.0        0.0     170.0   205.0    0.0    NaN     0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rosetta</th>\n",
       "      <th>tsuboyama</th>\n",
       "      <th>fireprot</th>\n",
       "      <th>q3421</th>\n",
       "      <th>s461</th>\n",
       "      <th>ssym</th>\n",
       "      <th>korpm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>korpm</th>\n",
       "      <td>956.0</td>\n",
       "      <td>13814.0</td>\n",
       "      <td>4245.0</td>\n",
       "      <td>2897.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>596.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rosetta</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2354.0</td>\n",
       "      <td>3152.0</td>\n",
       "      <td>1928.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>1406.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsuboyama</th>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1031.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>901.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fireprot</th>\n",
       "      <td>1011.0</td>\n",
       "      <td>6324.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3310.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>1936.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q3421</th>\n",
       "      <td>1017.0</td>\n",
       "      <td>7123.0</td>\n",
       "      <td>4896.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.0</td>\n",
       "      <td>618.0</td>\n",
       "      <td>1920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s461</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4224.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssym</th>\n",
       "      <td>333.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>275.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rosetta  tsuboyama  fireprot   q3421   s461   ssym   korpm\n",
       "korpm        956.0    13814.0    4245.0  2897.0  360.0  596.0     NaN\n",
       "rosetta        NaN     2354.0    3152.0  1928.0   11.0  554.0  1406.0\n",
       "tsuboyama     33.0        NaN    1031.0   130.0  163.0    0.0   901.0\n",
       "fireprot    1011.0     6324.0       NaN  3310.0   71.0  382.0  1936.0\n",
       "q3421       1017.0     7123.0    4896.0     NaN   71.0  618.0  1920.0\n",
       "s461           4.0     4224.0      51.0    52.0    NaN    0.0   147.0\n",
       "ssym         333.0        0.0    1006.0   926.0    0.0    NaN   275.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pslm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
